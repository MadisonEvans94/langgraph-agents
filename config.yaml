llm_configs:
  default_llm:
    api_key: null # Will be overridden by .env
    base_url: "http://vllm-downstream:4882/v1"
    model_id: "meta-llama/Llama-3.1-8B-Instruct"
    max_new_tokens: 512
    temperature: 1.0
    top_p: 1.0
    repetition_penalty: 1.0
  llm2:
    api_key: null # Will be overridden by .env
    base_url: "http://vllm-downstream-2:4882/v1"
    model_id: "meta-llama/Llama-3.1-8B-Instruct"
    max_new_tokens: 512
    temperature: 1.0
    top_p: 1.0
    repetition_penalty: 1.0
